import streamlit as st
import os
import hashlib
import chromadb
import google.generativeai as genai
import pandas as pd # Para Excel y CSV
import docx # Para Word 
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

# ============================================================
# CONFIGURACI√ìN GENERAL
# ============================================================
st.set_page_config(page_title="Chat Multi-Documento con Gemini")

# Carga variables de entorno desde .env
# Aqu√≠ se espera GOOGLE_API_KEY=xxxx
load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Modelo de embeddings local
# Se puede cambiar por otros modelos de sentence-transformers
EMBEDDING_MODEL = SentenceTransformer("all-MiniLM-L6-v2")

# Se Inicializa el Cliente de ChromaDB
client = chromadb.Client()

# ============================================================
# SESSION STATE
# ============================================================
# session_state nos permite "recordar" cosas entre reruns.
if "collection" not in st.session_state:
    st.session_state.collection = None

if "file_processed" not in st.session_state:
    st.session_state.file_processed = False

if "file_hash" not in st.session_state:
    st.session_state.file_hash = None

# ============================================================
# FUNCIONES
# ============================================================
def hash_file(file) -> str:
    return hashlib.sha256(file.getvalue()).hexdigest()

def extract_text(pdf_file):
    """
    Extrae texto de un PDF digital (no escaneado).
    Incluye el n√∫mero de p√°gina como marcador.
    """
    file_extension = uploaded_file.name.split('.')[-1].lower()
    text = ""

    try:
        if file_extension == "pdf":
            reader = PdfReader(uploaded_file)
            for i, page in enumerate(reader.pages):
                content = page.extract_text()
                if content:
                    text += f"\n[P√°gina {i+1}]\n{content}"

        elif file_extension == "docx":
            doc = docx.Document(uploaded_file)
            text = "\n".join([para.text for para in doc.paragraphs])

        elif file_extension == "txt":
            text = uploaded_file.read().decode("utf-8")

        elif file_extension in ["xlsx", "xls"]:
            df = pd.read_excel(uploaded_file)
            # Convertimos el DataFrame a una cadena de texto legible
            text = "Datos de la tabla:\n" + df.to_string(index=False)

        elif file_extension == "csv":
            df = pd.read_csv(uploaded_file)
            text = "Datos del CSV:\n" + df.to_string(index=False)
            
    except Exception as e:
        st.error(f"Error al procesar el archivo: {e}")
        return None

    return text


def chunk_text(text):
    """
    Divide un texto largo en fragmentos (chunks) con solapamiento.

    chunk_size:
        - N√∫mero m√°ximo de caracteres por fragmento
        - Valores t√≠picos: 400‚Äì800
        - M√°s grande = m√°s contexto, pero embeddings m√°s caros

    overlap:
        - N√∫mero de caracteres que se repiten entre chunks consecutivos
        - Evita que una idea quede cortada entre fragmentos
        - Regla com√∫n: 10‚Äì20% del chunk_size

    Devuelve:
        Lista de diccionarios, cada uno representando un chunk con:
        - id           -> identificador √∫nico
        - content      -> texto del fragmento
        - start_index  -> posici√≥n donde comienza en el texto original
        - size         -> longitud real del chunk
    """
    chunk_size = 500 
    overlap = 100
    chunks = []          # Aqu√≠ guardaremos todos los fragmentos
    start = 0            # Puntero que indica desde d√≥nde empezamos a cortar
    chunk_id = 0         # Contador para asignar IDs √∫nicos

    # El while se ejecuta mientras NO hayamos llegado al final del texto
    while start < len(text):

        # 1Ô∏è‚É£ Cortamos el texto desde 'start' hasta 'start + chunk_size'
        #    Python corta autom√°ticamente si se pasa del largo del texto
        chunk_text = text[start:start + chunk_size]

        # 2Ô∏è‚É£ Guardamos el chunk junto con metadata √∫til
        chunks.append({
            "id": f"chunk_{chunk_id}",   # Identificador √∫nico del fragmento
            "content": chunk_text,       # Texto real del fragmento
            "start_index": start,        # Posici√≥n en el texto original
            "size": len(chunk_text)      # Tama√±o real del fragmento
        })

        # 3Ô∏è‚É£ Incrementamos el ID para el pr√≥ximo chunk
        chunk_id += 1

        # 4Ô∏è‚É£ Avanzamos el puntero 'start'
        #    No avanzamos chunk_size completo,
        #    sino (chunk_size - overlap) para que haya solapamiento
        #
        #    Ejemplo:
        #    chunk_size = 500
        #    overlap    = 100
        #    start avanza 400 caracteres
        #
        #    Los √∫ltimos 100 caracteres del chunk actual
        #    aparecer√°n tambi√©n al inicio del siguiente
        start += chunk_size - overlap

    # 5Ô∏è‚É£ Cuando start >= len(text), el while termina
    #    y devolvemos todos los fragmentos creados
    return chunks



def create_chroma_collection(chunks):
    """
    Crea una colecci√≥n nueva en ChromaDB a partir de los chunks generados.

    Cada chunk se almacena junto con:
    - su embedding (vector num√©rico)
    - su texto original
    - metadata √∫til
    """

    # ------------------------------
    # 1Ô∏è‚É£ Borrado defensivo
    # ------------------------------
    # Si ya existe una colecci√≥n con el mismo nombre ("pdf_rag"),
    try:
        client.delete_collection("multi_doc_rag")
    except:
        # Si la colecci√≥n no existe, Chroma lanza error.
        # Lo ignoramos porque es un caso esperado.
        pass

    # ------------------------------
    # 2Ô∏è‚É£ Crear colecci√≥n nueva
    # ------------------------------
    # Aqu√≠ Chroma crea:
    # - una tabla de documentos
    # - un √≠ndice vectorial
    # - espacio para metadatos
    collection = client.create_collection(name="multi_doc_rag")

    # ------------------------------
    # 3Ô∏è‚É£ Separar texto de metadata
    # ------------------------------
    # Extraemos SOLO el contenido textual de cada chunk.
    # Esto es lo que se convertir√° en embeddings.
    texts = [c["content"] for c in chunks]

    # ------------------------------
    # 4Ô∏è‚É£ Generar embeddings
    # ------------------------------
    # El modelo de SentenceTransformers convierte cada texto
    # en un vector num√©rico.
    #
    # Cada vector representa el significado del chunk.
    embeddings = EMBEDDING_MODEL.encode(texts)

    # ------------------------------
    # 5Ô∏è‚É£ Insertar datos en Chroma
    # ------------------------------
    collection.add(
        # Texto original del chunk
        documents=texts,

        # Vectores que permiten b√∫squeda sem√°ntica
        embeddings=embeddings.tolist(),

        # IDs √∫nicos
        # Sirven para identificar cada chunk internamente
        ids=[c["id"] for c in chunks],

        # Metadata asociada a cada chunk
        metadatas=[
            {
                "chunk_index": i,         # Orden del chunk
                "start_index": c["start_index"],  # Posici√≥n en el texto original
                "chunk_size": c["size"]   # Tama√±o real del fragmento
            }
            for i, c in enumerate(chunks)
        ]
    )

    # ------------------------------
    # 6Ô∏è‚É£ Devolver colecci√≥n lista
    # ------------------------------
    # La colecci√≥n ya puede:
    # - recibir queries (preguntas)
    # - devolver chunks relevantes
    return collection



def retrieve_context(collection, query, k=4):
    """
    Recupera los k chunks m√°s similares a la pregunta.
    Devuelve tanto el texto como la metadata asociada.
    """
    query_embedding = EMBEDDING_MODEL.encode([query])

    results = collection.query(
        query_embeddings=query_embedding.tolist(),
        n_results=k
    )

    return results


def ask_gemini(context, question):
    """
    Llama a Gemini usando el contexto recuperado.
    El prompt fuerza comportamiento RAG (no inventar).
    """
    model = genai.GenerativeModel("models/gemini-2.5-flash-lite")

    prompt = f"""
Eres un asistente que responde SOLO con la informaci√≥n del contexto.
Si la respuesta no est√° en el contexto, di: "No se encuentra en el documento".

Contexto:
{context}

Pregunta:
{question}
"""

    response = model.generate_content(prompt)
    return response.text

# ============================================================
# INTERFAZ
# ============================================================

st.title("üìÇ Chat Multi-Documento Inteligente")

uploaded_file = st.file_uploader(
    "Selecciona un archivo", 
    type=["pdf", "docx", "txt", "xlsx", "csv"]
)

# üîÑ Detectar cambio de PDF y resetear estado
if uploaded_file:
    current_hash = hash_file(uploaded_file)

    if st.session_state.file_hash != current_hash:
        st.session_state.file_hash = current_hash
        st.session_state.file_processed = False
        st.session_state.collection = None

# ------------------------------
# BOT√ìN PROCESAR PDF
# ------------------------------
if uploaded_file and not st.session_state.file_processed:
    if st.button("üì• Procesar Documento"):
        with st.spinner("Analizando contenido..."):
            raw_text = extract_text(uploaded_file)
            if raw_text:
                chunks = chunk_text(raw_text)
                st.session_state.collection = create_chroma_collection(chunks)
                st.session_state.file_processed = True
                st.success(f"¬°Listo! Documento fragmentado en {len(chunks)} partes.")

# ------------------------------
# SECCI√ìN DE PREGUNTAS
# ------------------------------
if st.session_state.file_processed and st.session_state.collection:
    st.divider()

    question = st.text_input("¬øQu√© deseas saber sobre el archivo?")

    if st.button("ü§ñ Preguntar") and question:
        with st.spinner("Buscando respuesta..."):
            results = retrieve_context(st.session_state.collection, question)

            # Unimos los documentos para Gemini
            context_text = "\n\n".join(results["documents"][0])

            answer = ask_gemini(context_text, question)

        st.subheader("ü§ñ Respuesta")
        st.write(answer)

        # ------------------------------
        # DETALLE DEL CONTEXTO USADO
        # ------------------------------
        with st.expander("üìö Contexto usado (detallado)"):
            for i, (doc, meta) in enumerate(
                zip(results["documents"][0], results["metadatas"][0])
            ):
                st.markdown(f"""
**Chunk #{meta['chunk_index']}**
- üìç Inicio en texto: `{meta['start_index']}`
- üìè Tama√±o: `{meta['chunk_size']}` caracteres

```text
{doc}
""")